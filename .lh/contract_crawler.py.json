{
    "sourceFile": "contract_crawler.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1711088399614,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1711088399614,
            "name": "Commit-0",
            "content": "# python contract_crawler.py --web bscscan --session-url https://bscscan.com/address/0x2D530a3b07F2a9Cc3B9043356Af293aEE09ED103#code\n# python contract_crawler.py --web polygon --use-api --api-key $POLYGONSCAN_API_KEY --csv ../py-scripts/polygon.addresses.csv\n\nimport argparse\nimport os\nimport fnmatch\nimport json\nfrom typing import Optional, Dict, List, Any\n\nfrom bs4 import BeautifulSoup\nimport requests\nfrom datetime import datetime\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\nproxies = {}  #{'http': \"socks5://127.0.0.1:1080\", 'https': \"socks5://127.0.0.1:1080\"}\n\nREQ_HEADER = {\n    'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n}\n\nVERIFIED_CONTRACT_URL = 'https://etherscan.io/contractsVerified'\nCONTRACT_SOURCE_URL   = 'https://etherscan.io/address/{}#code'\n\n\nBASE_URL_BY_NETWORK ={\n    'etherscan': 'https://api.etherscan.io/api',\n    'bscscan': 'https://api.bscscan.com/api',\n    'polygon': 'https://api.polygonscan.com/api'\n}\n\nINPAGE_META_TEXT = {'Contract Name:': 'contract_name',\n                    'Compiler Version': 'version',\n                    'Optimization Enabled': 'optimizations',\n                    'Other Settings:': 'settings'}\n\n\ndef to_path_name(s, max_length=255):\n    s = re.sub(r'[^a-zA-Z0-9\\-_\\.]', '_', s)\n    return s[:max_length]\n\ndef starts_with_digit(s):\n    return bool(re.match(r'^\\d', s))\n\nsession = {}\n\nclass any_of_elements_present:\n    def __init__(self, *locators):\n        self.locators = locators\n\n    def __call__(self, driver):\n        for locator in self.locators:\n            try:\n                element = EC.presence_of_element_located(locator)(driver)\n                if element:\n                    return element\n            except:\n                pass\n        return False\n\ndef catch_all_exceptions(handler):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                return handler(e)\n        return wrapper\n    return decorator\n\ndef exception_silent_handler(e):\n    # print(f\"Caught an exception: {str(e)}\")\n    return None\n\n\ndef get_session_from_chromedriver(url):\n    options = uc.ChromeOptions()\n    options.add_argument('--no-sandbox')\n    options.add_argument('--disable-gpu')\n    # options.add_argument('--headless=new')\n\n    # options.add_argument('--headless')\n\n    # driver = uc.Chrome(options=options)\n    driver = uc.Chrome(options=options, browser_executable_path='/usr/bin/brave', enable_cdp_events=True, version_main=116, force=True)\n\n    user_agent = driver.execute_script(\"return navigator.userAgent;\")\n\n    selectors = [\n        '#ctl00 > div.d-md-flex.justify-content-between.my-3 > ul',\n        '#ContentPlaceHolder1_pageRecords > nav > ul',\n        '#searchFilterInvoker',\n        '#ContentPlaceHolder1_li_transactions'\n    ]\n\n    selectors = [(By.CSS_SELECTOR, s) for s in selectors]\n\n    driver.get(url)\n    WebDriverWait(driver, 30).until(any_of_elements_present(*selectors))\n    cookies = driver.get_cookies()\n\n    print(cookies)\n\n    session = requests.Session()\n    session.headers.update({'User-Agent': user_agent})\n\n    if len(cookies) < 1:\n        raise Exception('Should have some cookies here')\n\n    for cookie in cookies:\n        session.cookies.set(cookie['name'], cookie['value'])\n\n    print(f'Cookies loaded from {url} {session.cookies}')\n    return session\n\ndef load_session(url):\n    global session\n    if not session:\n        session = get_session_from_chromedriver(url)\n    return session\n\n\ndef address_from_tr(td: Any) -> str:\n    a = td.select_one('a.js-clipboard')\n    return a.attrs.get('data-clipboard-text') if (a and a.attrs) else None\n\n# Crawl meta info of contracts by page\ndef parse_page(page: Optional[int]=None, retry=3, retry_delay=5) -> Optional[List[Dict[str, str]]]:\n    url = VERIFIED_CONTRACT_URL if page is None else f'{VERIFIED_CONTRACT_URL}/{page}'\n    print(f'Crawling {url}')\n    resp = session.get(url, allow_redirects=False)\n    if resp.status_code != 200:\n        print(f'No results found on page: {page}, http status: {resp.status_code}')\n        return None\n    try:\n        soup = BeautifulSoup(resp.content, 'lxml')\n        trs = soup.select('tr')\n        table_headers = [th.text.strip() for th in trs[0].select('th')]\n        return [dict(zip(table_headers, [address_from_tr(td) or td.text.strip() for td in tr.select('td')])) for tr in trs[1:]]\n    except Exception as e:\n        print(f'Error {e}')\n        if retry > 0:\n            time.sleep(retry_delay)\n            f'Parse page failed for {url}, status {resp.status_code}, retry in {retry_delay} secs'\n            return parse_page(page, retry-1, retry_delay)\n        else:\n            raise e\n\ndef parse_for_balance(soup):\n    bsc_selector = '#ContentPlaceHolder1_divSummary > div.row.mb-4 > div.col-md-6.mb-3.mb-md-0 > div > div.card-body > div:nth-child(1) > div.col-md-8'\n    eth_selector= '#ContentPlaceHolder1_divSummary > div.row.g-3.mb-4 > div:nth-child(1) > div > div > div:nth-child(2) > div > div'\n    found = soup.select_one(eth_selector) or soup.select_one(bsc_selector)\n    return found.text.strip() if found else None\n\ndef parse_for_num_txs(soup):\n    bsc_selector = '#transactions > div.d-md-flex.align-items-center > p'\n    eth_selector = '#ContentPlaceHolder1_divTxDataInfo > div > p'\n\n    found = soup.select_one(eth_selector) or soup.select_one(bsc_selector)\n    if not found:\n        return None\n\n    s = found.text.strip()\n    match = re.search(r'a total of ([\\d,]+)', s)\n\n    if match:\n        num_str = match.group(1)\n        num_str = num_str.replace(',', '')\n        return int(num_str)\n    else:\n        return None\n\n# Parse meta data from source code page\ndef parse_for_inpage_meta(soup):\n    rows = [t.text.strip().split('\\n', maxsplit=1) for t in soup.select('#ContentPlaceHolder1_contractCodeDiv .row div')]\n    # rows = [t.text.strip().split('\\n+', maxsplit=1) for t in soup.select('#ContentPlaceHolder1_contractCodeDiv .row div')]\n    rows = [[t[0].strip(), t[1].strip()] for t in rows if len(t) == 2]\n    rows = [(INPAGE_META_TEXT[t[0]], t[1]) for t in rows if t[0] in INPAGE_META_TEXT]\n    balance = parse_for_balance(soup)\n    num_txs = parse_for_num_txs(soup)\n    data = dict(rows)\n\n    if balance:\n        data['balance'] = balance\n\n    if num_txs:\n        data['num_txs'] = num_txs\n\n    return data\n\ndef parse_for_contract_name(soup):\n    meta = parse_for_inpage_meta(soup)\n    return meta['contract_name']\n\ndef select_file_names(soup) -> list:\n    @catch_all_exceptions(exception_silent_handler)\n    def parse_for_file_name(text):\n        num_text, name_text = text.strip().split(':')\n        _, n, _, total = num_text.split()\n        num = f'{n:0>2}_{total:0>2}'\n        return f'{num}_{name_text.strip()}'\n\n    selectors  =  ['.d-flex > .text-secondary', '.d-flex > .text-muted', '#dividcode > div > span']\n    file_spans = []\n    files = []\n    for selector in selectors:\n        file_spans = soup.select(selector)\n        files = [parse_for_file_name(name.text) for name in file_spans]\n        files = [f for f in files if f]\n        if len(files) > 0:\n            break\n    return files\n\ndef select_sources(soup) -> list:\n    selectors = ['.js-sourcecopyarea', 'div.ace_scroller > div', 'pre.editor']\n    sources = []\n    for selector in selectors:\n        sources = [e.text for e in soup.select(selector) if e.text and not starts_with_digit(e.text)]\n        if len(sources) > 0:\n            break\n    return sources\n\ndef parse_source_soup(soup, address=None, contract_name=None):\n    address = address or soup.select('title')[0].text.split(r'|')[1].strip().split()[-1]\n    contract_name = contract_name or parse_for_contract_name(soup) or ''\n\n    safe_contract_name = to_path_name(contract_name)\n\n    if not contract_name:\n        print(f'ERROR: No contract name found in {address}')\n\n    parent = f'{ROOT_DIR}/{address}_{safe_contract_name}'\n    os.makedirs(parent, exist_ok=True)\n\n\n    def write_source_file(source_file_name, source_code, overwrite=False):\n        f = f'{parent}/{source_file_name}'\n        if (not overwrite) and os.path.exists(f):\n            return\n        print(f'Saving {f}')\n        with open(f, 'w') as f:\n            f.write(source_code)\n\n    files = select_file_names(soup)\n    sources = select_sources(soup)\n\n\n    if len(sources) < 1:\n        raise Exception(f'No source code found for {address} {contract_name}')\n\n    inpage_meta = parse_for_inpage_meta(soup)\n    write_source_file(f'inpage_meta.json', json.dumps(inpage_meta), True)\n\n    if len(sources) > 1 and len(files) == 0:\n        raise Exception(f'Multiple source with no file name? {address} {contract_name}')\n\n    if len(sources) == 1 and len(files) == 0:\n        write_source_file(f'{safe_contract_name}.sol', sources[0])\n        return\n\n    # if len(sources) != len(files): the extra source appearing in the sources might just be `settings`\n    #     raise Exception(f'Number of files does not match number of source text: {address} {contract_name}')\n\n    for source_file_name, source_code in zip(files, sources):\n        write_source_file(source_file_name, source_code)\n\n# Save metadata of a contract to a json file\ndef write_meta_json(contract: Dict[str, str]):\n    address = contract['Address']\n    contract_name = contract['Contract Name']\n\n    if not (address and contract_name):\n        raise Exception(f'Bad meta data in {contract}')\n\n    parent = f'{ROOT_DIR}/{address}_{contract_name}'\n    os.makedirs(parent, exist_ok=True)\n    f = f'{parent}/meta.json'\n\n    if not os.path.exists(f):\n        with open(f, 'w') as f:\n            f.write(json.dumps(contract, indent=2))\n\n# Get contract source code\ndef download_source(contract: Dict[str, str], retry=3, retry_delay=5, throw_if_fail=False) -> None:\n    address = contract['Address']\n    contract_name = contract['Contract Name']\n    url = CONTRACT_SOURCE_URL.format(address)\n    print(f\"downloading {url}\")\n    resp = session.get(url, allow_redirects=False)\n\n    def maybe_retry(e=None):\n        if retry > 0:\n            time.sleep(retry_delay)\n            f'Download source failed for {address} {contract_name}, status {resp.status_code}, retry in {retry_delay} secs'\n            return download_source(contract, retry-1, retry_delay)\n        else:\n            if throw_if_fail:\n                raise e or Exception(f'Download source abort for {address} {contract_name}, status {resp.status_code}')\n            return\n\n    if resp.status_code != 200:\n        maybe_retry()\n\n    try:\n        soup = BeautifulSoup(resp.content, 'lxml')\n        parse_source_soup(soup, address, contract_name)\n    except Exception as e:\n        maybe_retry(e)\n\ndef fetch_all():\n    contracts = [c for p in range(1, 21) for c in parse_page(p)]\n    now = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n    with open(f'{ROOT_DIR}/contracts_{now}.json', 'w') as f:\n        f.write(json.dumps(contracts, indent=2))\n\n    for contract in contracts:\n        write_meta_json(contract)\n        download_source(contract)\n\ndef download_url_poly(url, retry=3, retry_delay=5, throw_if_fail=False):\n    address = url.split('/')[-1].split('#')[0]\n\n    session = get_session_from_chromedriver(url)\n\n    resp = session.get(url)\n    soup = BeautifulSoup(resp.content, 'lxml')\n    parse_source_soup(soup, address)\n\ndef download_url(url, retry=3, retry_delay=5, throw_if_fail=False):\n    address = url.split('/')[-1].split('#')[0]\n    resp = session.get(url, allow_redirects=False)\n\n    if resp.status_code != 200:\n        if retry > 0:\n            time.sleep(retry_delay)\n            f'Download source failed for {url}, status {resp.status_code}, retry in {retry_delay} secs'\n            return download_url(url, retry-1, retry_delay)\n        else:\n            if throw_if_fail:\n                raise Exception(f'Download source abort for {url}, status {resp.status_code}')\n            return\n\n    soup = BeautifulSoup(resp.content, 'lxml')\n    try:\n        parse_source_soup(soup, address)\n    except Exception as e:\n        with open('.example.html', 'wb') as f:\n            f.write(resp.content)\n        raise e\n\ndef load_addresses_from_csv(csv_path: str, csv_col_index: int = 0):\n    import csv\n    with open(csv_path, 'r') as f:\n        reader = csv.reader(f)\n        return [row[csv_col_index].lower() for row in reader]\n\ndef build_existing_contracts(root: str):\n    return [f.split('_')[0].lower() for f in os.listdir(root) if os.path.isdir(os.path.join(root, f))]\n\ndef is_valid_ethereum_address(address):\n    pattern = \"^0x[a-fA-F0-9]{40}$\"\n    return bool(re.match(pattern, address))\n\n\n@sleep_and_retry\n@limits(calls=5, period=1)\ndef throttled_get(url):\n    return requests.get(url)\n\ndef retrieve_standard_json_input_by_api(network, api_key, address, root):\n    if os.path.exists(root):\n        return\n\n    base_url = BASE_URL_BY_NETWORK.get(network)\n    url = f'{base_url}?module=contract&action=getsourcecode&address={address}&apikey={api_key}'\n    response = requests.get(url)\n    data = json.loads(response.text)\n    if data['status'] == '1':\n        result = data['result'][0]\n        contract_name = result.get('ContractName')\n        if not contract_name:\n            print(f'Contract name not found for {address}')\n            return\n        output = os.path.join(root, f'{address}_{to_path_name(contract_name, 20)}.json')\n        os.mkdir(root)\n        with open(output, 'w') as f:\n            print(output)\n            json.dump(result, f)\n\n\n\n\nif __name__ == '__main__':\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--web\", default=\"etherscan\",type=str, help=\"Choose website, etherscan(default) or bscscan, polygon\")\n    ap.add_argument(\"--url\", type=str, help=\"URL of contract to download\")\n    ap.add_argument(\"--output-dir\", type=str, help=\"URL of contract to download\", default=\"./\")\n    ap.add_argument(\"--session-url\", type=str, help=\"URL to load the first session from\")\n    ap.add_argument(\"--csv\", type=str, help=\"Load address from csv file\")\n    ap.add_argument(\"--csv-col-index\", type=int, help=\"Column index of address in csv file\", default=-1)\n    ap.add_argument(\"--use-api\", action='store_true', help=\"Use API to download source code\")\n    ap.add_argument(\"--address\", type=str, help=\"Address to download (to be used with the `--use-api` flag)\")\n    ap.add_argument(\"--api-key\", type=str, help=\"API key for etherscan or bscscan or polygonscan\")\n\n    args = ap.parse_args()\n    OUTPUT_DIR = args.output_dir\n    OUTPUT_DIR = OUTPUT_DIR[:-1] if OUTPUT_DIR.endswith('/') else OUTPUT_DIR\n    ROOT_DIR = f'{OUTPUT_DIR}/contracts'\n\n    addresses = set()\n    if args.csv:\n        addresses = set(load_addresses_from_csv(args.csv, args.csv_col_index))\n        print(f'Found {len(addresses)} addresses in {args.csv}')\n    elif args.address:\n        addresses = set([args.address])\n\n    if args.web == 'etherscan':\n        VERIFIED_CONTRACT_URL = 'https://etherscan.io/contractsVerified'\n        CONTRACT_SOURCE_URL   = 'https://etherscan.io/address/{}#code'\n        os.makedirs(ROOT_DIR, exist_ok=True)\n        fn = download_url\n\n    elif args.web == 'bscscan':\n        VERIFIED_CONTRACT_URL = 'https://bscscan.com/contractsVerified'\n        CONTRACT_SOURCE_URL   = 'https://bscscan.com/address/{}#code'\n        ROOT_DIR = f'{OUTPUT_DIR}/bsc_contracts'\n        os.makedirs(ROOT_DIR, exist_ok=True)\n        fn = download_url\n\n    elif args.web == \"polygon\":\n        VERIFIED_CONTRACT_URL = 'https://polygonscan.com/contractsVerified'\n        CONTRACT_SOURCE_URL   = 'https://polygonscan.com/address/{}#code'\n        ROOT_DIR = f'{OUTPUT_DIR}/polygon_contracts'\n        os.makedirs(ROOT_DIR, exist_ok=True)\n        fn = download_url_poly\n\n    else:\n        raise Exception('Invalid website, choose etherscan or bscscan')\n\n    print(VERIFIED_CONTRACT_URL)\n\n    url = args.url\n\n    if not args.use_api:\n        load_session(args.session_url or VERIFIED_CONTRACT_URL)\n\n    if url:\n        fn(url)\n    elif args.csv or args.address:\n        existing = build_existing_contracts(ROOT_DIR)\n        processed = set()\n        if os.path.exists('.processed.txt'):\n            with open('.processed.txt', 'r') as f:\n                lines = [line.lower() for line in f.read().split('\\n')]\n                processed = set(processed)\n\n        ignored = processed.union(existing)\n        f_processed = open('.processed.txt', 'w')\n        addresses = addresses.difference(ignored)\n\n        print(f'Found {len(addresses)} addresses to process')\n        for address in addresses:\n            if not is_valid_ethereum_address(address):\n                print(f'Invalid address {address}')\n                continue\n\n            url = CONTRACT_SOURCE_URL.format(address)\n            print(f'Processing {url}')\n            try:\n                if args.use_api:\n                    retrieve_standard_json_input_by_api(args.web, args.api_key, address, f'{ROOT_DIR}/{address}')\n                else:\n                    fn(url)\n            except KeyError as e:\n                if 'contract_name' in str(e):\n                    print(f'Probably no code found for {address}')\n                else:\n                    raise e\n            finally:\n                f_processed.write(f'{address}\\n')\n                f_processed.flush()\n\n    else:\n        fetch_all()\n\n    print(\"all jobs done\")\n"
        }
    ]
}